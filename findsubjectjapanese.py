# -*- coding: utf-8 -*-
"""FindSubjectJapanese.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y5qTM77QdmvNsDFovyqgyDnXJ9Pguxoo

"""

import nltk
import string
import math
import numpy
import matplotlib
import re
import spacy
import requests, os

import ginza

nlp = spacy.load("ja_ginza")

import torch
from transformers import BertJapaneseTokenizer, BertForMaskedLM
#model_name = "cl-tohoku/bert-base-japanese-whole-word-masking"
model_name = "tohoku-nlp/bert-base-japanese-v3"
tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)
bert_model = BertForMaskedLM.from_pretrained(model_name)



from transformers import AutoModelForTokenClassification,  BertTokenizer, AutoModelForSequenceClassification

modelAddMASK = AutoModelForTokenClassification.from_pretrained(
  "models/subject_insertion_model"
)

tokenizerAddMASK = BertTokenizer.from_pretrained(
  "models/subject_insertion_model"
)

modelParticle = AutoModelForSequenceClassification.from_pretrained(
  "models/particle_prediction_model"
)

"""Objetivo: Dado un párrafo en japonés se debe predecir el sujeto de la oración seleccionada.

Funciones auxiliares para el preprocesamiento del texto a analizar
"""

# Auxiliary function to split a Japanese text into sentences based on punctuation marks
def sentence_spliter(text):

  japanese_sentence_punctuations = '。？！'

  sentences = re.findall(f".*?[{japanese_sentence_punctuations}]", text)

  if sentences and not text.endswith(sentences[-1]):
      remaining_text = text[text.rfind(sentences[-1]) + len(sentences[-1]):]
      if remaining_text.strip():
          sentences.append(remaining_text)
  elif not sentences and text.strip():
      sentences.append(text)

  return sentences

#Auxiliary function to find the subject of a sentence using the dependency tree provided by spacy + ginza. We look for the nominal subject (nsubj) that is directly related to the root of the sentence, which is usually the main verb. If we find such a subject, we return it; otherwise, we return None.
def find_obl(doc):
  obl_candidates = []
  root = 0
  bunsetsu_spans = ginza.bunsetu_spans(doc)
  candidate = None
  for i, bunsetsu in enumerate(bunsetsu_spans):
    print(f"Bunsetsu {i+1}: {bunsetsu.text}")
    for token in bunsetsu:
      #  print(token.i,token.orth_,token.lemma_,token.norm_,token.morph.get("Reading"),token.pos_,token.morph.get("Inflection"),token.tag_,token.dep_,token.head.i,)
        if(token.dep_ == "ROOT"):
          root = token.i
        if ((token.dep_ == "nsubj") & ((token.pos_ == "PRON") | (token.pos_ == "PROPN") | (token.pos_ == "NOUN"))):
          obl_candidates.append(token)
   # print("-" * 20)
  #dot = visualize_spacy_dependency_tree(doc)
  #display(dot)
  for token in obl_candidates:
    #print(token.i,token.orth_,token.lemma_,token.norm_,token.morph.get("Reading"),token.pos_,token.morph.get("Inflection"),token.tag_,token.dep_,token.head.i,)
    if(token.head.i == root):
      return  root, token
  return  root, None

from graphviz import Digraph
from IPython.display import display

#Auxiliar function to graphically visualize the dependency tree of a sentence using graphviz
def visualize_spacy_dependency_tree(doc):
   
    dot = Digraph(comment=f'Sentence: {doc.text}')
    dot.attr('node', shape='ellipse')

    # Add nodes (words) to the graph
    for token in doc:
        label = f"{token.text} ({token.pos_})"
        dot.node(str(token.i), label) # Use token.i as a unique identifier for the node

    # Add edges (dependencies) to the graph
    for token in doc:
        if token.head != token: # Avoid drawing a self-loop for the root
            dot.edge(str(token.head.i), str(token.i), label=token.dep_)

    return dot



#Auxiliar function to show the dependency tree and the morphological information of each token in a sentence using spacy + ginza. This function is useful for debugging and understanding the structure of the sentence we are analyzing.
def show_info(doc):
  bunsetsu_spans = ginza.bunsetu_spans(doc)
  for i, bunsetsu in enumerate(bunsetsu_spans):
    print(f"Bunsetsu {i+1}: {bunsetsu.text}")
    for token in bunsetsu:
        print(
            token.i,
            token.orth_,
            token.lemma_,
            token.norm_,
            token.morph.get("Reading"),
            token.pos_,
            token.morph.get("Inflection"),
            token.tag_,
            token.dep_,
            token.head.i,
        )
    print("-" * 20)
  dot = visualize_spacy_dependency_tree(doc)
  display(dot)

"""Pipeline functions that call the models"""

def mask_sentence(text):
  #First we use the model trained to predict the location of the subject to insert [MASK]
  tokens = [token.text for token in text]
  inputs = tokenizerAddMASK(tokens, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
  word_ids = inputs.word_ids(batch_index=0)
  with torch.no_grad():
    outputs = modelAddMASK(**inputs)
  logits = outputs.logits
  predictions = torch.argmax(logits, dim=2)

  # Align predictions back to original words
  aligned_predictions = []
  for i, word_id in enumerate(word_ids):
        if word_id is not None and (i == 0 or word_id != word_ids[i-1]):
            
            aligned_predictions.append('MASK' if predictions[0, i].item() == 1 else 'NOT_MASK')

    
  #We add MASK in the predicted location
  predicted_masked_sentence = []
  true_masked_sentence = []
  for i, token_form in enumerate(tokens):
      if i < len(aligned_predictions) and aligned_predictions[i] == 'MASK':
          predicted_masked_sentence.append('[MASK]')

      predicted_masked_sentence.append(token_form)

  print(f"Predecimos la ubicación del sujeto: {"".join(predicted_masked_sentence)}")
  
  #Then we use the model trained to predict the particle following the subject
  label_to_particle = {0:'は', 1: 'が', 2: 'も'}
  mask_input = tokenizerAddMASK(predicted_masked_sentence,is_split_into_words=True, return_tensors="pt", truncation=True, padding=True )
  with torch.no_grad():
    outputs = modelParticle(**mask_input)

  logits = outputs.logits
  predicted_label = torch.argmax(logits, dim=1).item()
  predicted_particle = label_to_particle[predicted_label]

  print(f"Predecimos la partícula: {predicted_particle}")
  

  result = ""
  for token_form in predicted_masked_sentence :
    result += token_form
    if token_form == '[MASK]':
      result += predicted_particle

  #we return the sentence with the predicted subject location and the predicted particle
  return result

#Finally, we use a pre-trained BERT model to predict the most probable word to fill the [MASK] token, which should correspond to the subject of the sentence. We show the top 5 predictions for that position.
def predict_subject(text):
  encoding = tokenizer(text,truncation=True, return_tensors="pt")
  output = bert_model(**encoding)
  #print(output[0].shape)
  mask_index = encoding["input_ids"][0].tolist().index(4)
  top_words = output[0][0][mask_index].topk(5).indices
  first_five_words = []
  for word_id in top_words:
    word = tokenizer.convert_ids_to_tokens(word_id.item())
    print(text.replace("[MASK]", word))
    first_five_words.append(word)
  return first_five_words

#Main function that implements the pipeline
def get_subject(text, sentence_index):
  #dividimos el texto en oraciones para analizar la oración seleccionada
  sentences = sentence_spliter(text)
  #Utilizamos spacy + ginza para detectar las relaciones sintácticas entre las palabras de esa oración
  doc = nlp(sentences[sentence_index])
  #Teniendo esta info, buscamos si exite un sujeto explícito
  verb, subject = find_obl(doc)

  print(f"Buscamos el sujeto de la oracion: {sentences[sentence_index]}")
  #Si existe lo mostramos
  if(subject != None):
    print("Sujeto explícito encontrado: ")
    print(subject.text)
    return subject.text
  else:
    #Si no existe primero utilizamos el modelo BERT preentrenado por mí para predecir el lugar donde debería estar el sujeto y allí insertamos [MASK]
    print("No se encontró sujeto explícito.")
    sentences[sentence_index] = "".join(mask_sentence(doc))
    print("Finalmente predecimos que palabra mejor remplazaria la forma:")
    print(sentences[sentence_index])
    text = "".join(sentences)
    #luego utilizamos un modelo MLM preentrenado para remplazar [MASK] por la cadena más probable
    predict_subject(text)
    return None

"""Probemos con un ejemplo"""

text = "検挙されておらず、2012年8月現在未解決。"

get_subject(text,0)
